# -*- coding: utf-8 -*-
"""TutorAI 1.2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hymu3KtR0Rdywwi41IdYFyPvLTRcK-Yl
"""

# @title üì¶ 1. Instala√ß√µes e importa√ß√µes
import sys
import subprocess
import os
import json
import gradio as gr
import re
import time

packages_to_install = [
    "transformers>=4.41.2",
    "torch",
    "sentence-transformers>=2.2.2",
    "faiss-cpu",
    "langchain-community>=0.2.0,<0.4.0",
    "langchain-huggingface>=0.1.0,<0.3.0",
    "unstructured[md]",
    "markdown",
    "gradio",
]

print("Instalando pacotes principais (pip resolver√° as vers√µes compat√≠veis)...")
install_cmd = [sys.executable, "-m", "pip", "install", "-q"] + packages_to_install
subprocess.check_call(install_cmd)
print("‚úÖ Instala√ß√£o conclu√≠da.\n")

print("Importando bibliotecas...")
try:
  # carregamento de documentos
  from langchain_community.document_loaders import UnstructuredMarkdownLoader
  from langchain.text_splitter import RecursiveCharacterTextSplitter

  # armazenamento vetorial e embeddings
  from langchain_community.vectorstores import FAISS
  from langchain_community.embeddings import HuggingFaceEmbeddings

  # modelo de linguagem e pipeline
  from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
  from langchain_community.llms import HuggingFacePipeline
  import torch

  # pra a cadeia RAG - Usando a API MODERNA
  from langchain_core.prompts import PromptTemplate
  from langchain.chains.combine_documents import create_stuff_documents_chain
  from langchain.chains.retrieval import create_retrieval_chain

  print("Importa√ß√µes conclu√≠das com sucesso.")

except ImportError as e:
  print(f"Erro ao importar bibliotecas: {e}")
  raise

print("\nAmbiente configurado.")

# @title üìÅ 2. Montagem do drive
# quando essa c√©lula for executada vai aparecer uma janela pedindo permiss√£o pra acessar o seu drive, voc√™ precisa aceitar pra continuar
# quando a c√©lula 2 for executada, vai aparecer uma janela pra que voc√™ conceda acesso ao seu driveQuando a c√©lula 2 for executada, vai aparecer uma janela pra que voc√™ conceda acesso ao seu drive

from google.colab import drive
drive.mount('/content/drive')
print("Google Drive montado em /content/drive")

# @title üîç Carregamento do Banco de Dados Vetorial (FAISS)
# O caminho pra pasta do FAISS vai depender de onde vc salvou.
# Uma vez que o drive esteja montado aqui, s√≥ ir no √≠cone da pasta na barra lateral direita, localizar onde voc√™ salvou a pasta, bot√£o direito, copiar caminho e colocar nas respectivas vari√°veis

caminho_pasta_faiss = "/content/drive/MyDrive/TutorAI_Colab" # esse caminho vai variar
subdiretorio_faiss = "faiss_index_tutorai"
caminho_faiss_index_completo = os.path.join(caminho_pasta_faiss, subdiretorio_faiss)
caminho_metadata = os.path.join(caminho_faiss_index_completo, "metadata.json")

try:
  # Verifica se os arquivos principais do √≠ndice FAISS existem
  arquivo_faiss = os.path.join(caminho_faiss_index_completo, "index.faiss")
  arquivo_pkl = os.path.join(caminho_faiss_index_completo, "index.pkl")

  if not os.path.exists(arquivo_faiss) or not os.path.exists(arquivo_pkl):
      print(f"√çndice FAISS ou arquivos principais (.faiss, .pkl) n√£o encontrados em '{caminho_faiss_index_completo}'.")
      print(f"  Procurando por: {arquivo_faiss} e {arquivo_pkl}")
      db = None
  else:
    print(f"Arquivos do √≠ndice FAISS encontrados em '{caminho_faiss_index_completo}'. Carregando...")

    # carrega os embeddings usados originalmente
    embeddings_model = "sentence-transformers/all-MiniLM-L6-v2"
    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model)

    # carrega o banco de dados vetorial FAISS
    # allow_dangerous_deserialization=True √© necess√°rio para carregar √≠ndices FAISS salvos
    db = FAISS.load_local(caminho_faiss_index_completo, embeddings, allow_dangerous_deserialization=True)

    print("Banco de dados vetorial carregado com sucesso do Google Drive!")

    # opcional: carregar e exibir metadados
    if os.path.exists(caminho_metadata):
        try:
            with open(caminho_metadata, 'r', encoding='utf-8') as f:
                metadata_salvo = json.load(f)
            print("\nMetadados carregados:")
            for key, value in metadata_salvo.items():
                print(f"  {key}: {value}")
        except json.JSONDecodeError:
            print(f"\nErro ao ler o arquivo de metadados '{caminho_metadata}' (formato inv√°lido).")
        except Exception as e_meta:
            print(f"\nErro inesperado ao carregar metadados: {e_meta}")
    else:
        print(f"\nArquivo de metadados '{caminho_metadata}' n√£o encontrado (opcional).")

except Exception as e:
  print(f"Erro ao carregar o banco de dados vetorial do Drive: {e}")
  db = None

# @title üß† Carregamento do Modelo TinyLlama (TinyLlama-1.1B-Chat-v1.0)

modelo = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

try:
  print(f"Carregando tokenizer do modelo: {modelo} ...")
  tokenizer = AutoTokenizer.from_pretrained(modelo, trust_remote_code=True)
  print("Tokenizer carregado.")

  print(f"Carregando modelo de linguagem: {modelo} ...")
  # carregar modelo com otimiza√ß√µes de mem√≥ria
  model = AutoModelForCausalLM.from_pretrained(
      modelo,
      device_map="auto", # distribui automaticamente
      torch_dtype=torch.float16, # usa float16 para reduzir uso de mem√≥ria
      trust_remote_code=True,
      low_cpu_mem_usage=True, # otimiza uso de RAM na CPU
      # attn_implementation="sdpa" # Opcional: pode acelerar em algumas GPUs
  )
  print("Modelo carregado.")

  # criar pipeline de texto para gera√ß√£o com o modelo TinyLlama
  # usando uma configura√ß√£o de gera√ß√£o simples
  pipe = pipeline(
      "text-generation",
      model=model,
      tokenizer=tokenizer,
      max_new_tokens=512,
      temperature=0.1,
      do_sample=False, # Greedy decoding para mais estabilidade
      eos_token_id=tokenizer.eos_token_id # Token de fim de sequ√™ncia
      # pad_token_id=tokenizer.eos_token_id # Evitar para DynamicCache
  )
  print("Pipeline HuggingFace (TinyLlama) criado.")

  llm = HuggingFacePipeline(pipeline=pipe)
  print(f"Modelo de linguagem '{modelo}' integrado ao LangChain com HuggingFacePipeline.")

except Exception as e:
  print(f"Erro ao carregar o modelo TinyLlama ou criar HuggingFacePipeline: {e}")
  print("Falha ao carregar o modelo TinyLlama com as configura√ß√µes atuais.")
  llm = None

# @title üß†üîó Cria√ß√£o da Cadeia RAG (Retrieval-Augmented Generation) - API Moderna (com TinyLlama)

if db and llm:
    template = """<|system|>
Voc√™ √© um assistente de estudos especializado em tecnologia, baseado em arquivos de documenta√ß√£o fornecidos.
Use as partes do contexto fornecido para responder √† pergunta no final.
Se voc√™ n√£o souber a resposta com base no contexto, diga que n√£o sabe, n√£o tente inventar uma resposta.
Mantenha a resposta concisa e baseada apenas no contexto fornecido.
<|end|>
<|user|>
Contexto: {context}

Pergunta: {input}

Resposta: <|assistant|>"""

    prompt = PromptTemplate(template=template, input_variables=["input", "context"])
    print("Template de prompt para TinyLlama definido.")

    # criar a cadeia para combinar documentos e gerar resposta
    combine_docs_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt
    )
    print("Cadeia de combina√ß√£o de documentos criada.")

    # criar a cadeia RAG completa (retrieval -> combine_docs)
    qa_chain = create_retrieval_chain(
        retriever=db.as_retriever(search_kwargs={"k": 4}), # N√∫mero de peda√ßos a recuperar
        combine_docs_chain=combine_docs_chain
    )
    print("Cadeia RAG (QA) criada com sucesso (com TinyLlama).")
else:
    print("A cadeia RAG n√£o pode ser criada. Verifique se o banco de dados (db) e o modelo (llm) est√£o configurados e s√£o v√°lidos.")
    print(f"db √© None: {db is None}")
    print(f"llm √© None: {llm is None}")
    qa_chain = None

# @title Teste da Cadeia RAG

if qa_chain:
    pergunta_usuario = "O que √© o black formatter em Python?" # Exemplo de pergunta

    print(f"üë§: {pergunta_usuario}")
    print("\n" + "="*60)
    print("Tutor AI:")

    try:
        # A nova API usa 'input' em vez de 'query'
        resposta_bruta = qa_chain.invoke({"input": pergunta_usuario})

        # --- Processamento para limpar a resposta ---
        resposta_completa = resposta_bruta['answer']
        # Encontra a √∫ltima ocorr√™ncia de <|assistant|> e pega o que vem depois
        # Isso isola a parte gerada pelo modelo como resposta
        inicio_resposta = resposta_completa.rfind("<|assistant|>")
        if inicio_resposta != -1:
            resposta_limpa = resposta_completa[inicio_resposta + len("<|assistant|>"):].strip()
        else:
            # Se <|assistant|> n√£o for encontrado, assume toda a resposta como v√°lida
            resposta_limpa = resposta_completa.strip()

        # --- Formata√ß√£o da resposta com estilo ASCII ---
        # Divide a resposta em linhas
        linhas_resposta = resposta_limpa.split('\n')
        for linha in linhas_resposta:
            # Adiciona um espa√ßo antes da linha para indentar levemente
            print(f" {linha}")

        # --- Exibi√ß√£o das fontes (opcional) ---
        print("\n" + "-"*40)
        print("Fontes utilizadas:")
        if resposta_bruta.get('context'): # Verifica se h√° contexto retornado
            for i, doc in enumerate(resposta_bruta['context'], start=1):
                caminho_arquivo = doc.metadata.get('source', 'Desconhecido')
                print(f" [{i}] {caminho_arquivo}")
        else:
            print(" Nenhuma fonte recuperada.")

    except Exception as e:
        print(f"\nErro durante a invoca√ß√£o da cadeia QA: {e}")

else:
    print("A cadeia RAG n√£o est√° configurada. Verifique as etapas anteriores.")

# @title üåê Interface Web (1.0)

def perguntar_interface(pergunta_usuario):
  """
  Fun√ß√£o que processa a pergunta do usu√°rio e retorna a resposta formatada e as fontes.
  Esta fun√ß√£o ser√° usada pelo Gradio.
  """
  if not qa_chain:
      return "Erro: A cadeia RAG n√£o est√° configurada.", ""

  try:
      resposta_bruta = qa_chain.invoke({"input": pergunta_usuario})

      # --- Processamento para limpar a resposta ---
      resposta_completa = resposta_bruta['answer']
      inicio_resposta = resposta_completa.rfind("<|assistant|>")
      if inicio_resposta != -1:
          resposta_limpa = resposta_completa[inicio_resposta + len("<|assistant|>"):].strip()
      else:
          resposta_limpa = resposta_completa.strip()

      # --- Formata√ß√£o da resposta para exibi√ß√£o ---
      # Pode-se adicionar mais limpezas ou formata√ß√µes aqui se necess√°rio
      resposta_formatada = resposta_limpa

      # --- Formata√ß√£o das fontes ---
      fontes_texto = "### Fontes Utilizadas\n\n"
      if resposta_bruta.get('context'):
          for i, doc in enumerate(resposta_bruta['context'], start=1):
              caminho_arquivo = doc.metadata.get('source', 'Desconhecido')
              fontes_texto += f"- [{i}] `{caminho_arquivo}`\n"
      else:
          fontes_texto += "- Nenhuma fonte recuperada.\n"

      return resposta_formatada, fontes_texto

  except Exception as e:
      erro_msg = f"Erro durante a invoca√ß√£o da cadeia QA: {e}"
      print(erro_msg) # Imprime no console do Colab tamb√©m, se houver erro
      return erro_msg, ""

# --- Configura√ß√£o da Interface do Gradio ---
with gr.Blocks(title="T√∫lioAI") as demo:
    gr.Markdown("# T√∫lioAI")
    gr.Markdown("Fa√ßa uma pergunta baseada nos seus arquivos de documenta√ß√£o.")

    # Coluna para agrupar input e bot√£o
    with gr.Column():
        user_input = gr.Textbox(label="Sua Pergunta", placeholder="Ex: O que √© o black formatter em Python?")
        submit_btn = gr.Button("Enviar")

    # Linha para os resultados, abaixo do input e bot√£o
    with gr.Row():
        with gr.Column():
            output_response = gr.Markdown(label="Resposta")
        with gr.Column():
            output_sources = gr.Markdown(label="Fontes")

    # Liga a fun√ß√£o ao clique do bot√£o
    submit_btn.click(
        fn=perguntar_interface,
        inputs=user_input,
        outputs=[output_response, output_sources]
    )

    # Liga a fun√ß√£o ao pressionar Enter no input
    user_input.submit(
        fn=perguntar_interface,
        inputs=user_input,
        outputs=[output_response, output_sources]
    )

# --- Lan√ßar a Interface ---
# share=True cria um link p√∫blico tempor√°rio
demo.launch(share=True)

# @title üåê Interface Web (1.1)

def perguntar_interface(pergunta_usuario):
  """
  Fun√ß√£o que processa a pergunta do usu√°rio e retorna a resposta formatada e as fontes.
  Esta fun√ß√£o ser√° usada pelo Gradio.
  """
  if not qa_chain:
      return "Erro: A cadeia RAG n√£o est√° configurada."

  try:
      resposta_bruta = qa_chain.invoke({"input": pergunta_usuario})

      # processamento para limpar a resposta ---
      resposta_completa = resposta_bruta['answer']
      inicio_resposta = resposta_completa.rfind("<|assistant|>")

      if inicio_resposta != -1:
          resposta_limpa = resposta_completa[inicio_resposta + len("<|assistant|>"):].strip()
      else:
          resposta_limpa = resposta_completa.strip()

      # formata√ß√£o da resposta para exibi√ß√£o ---
      resposta_formatada = resposta_limpa

      # formata√ß√£o das fontes ---
      fontes_texto = "\n\n---\n\n### Fontes Utilizadas\n\n"
      if resposta_bruta.get('context'):
          for i, doc in enumerate(resposta_bruta['context'], start=1):
              caminho_arquivo = doc.metadata.get('source', 'Desconhecido')
              fontes_texto += f"- [{i}] `{caminho_arquivo}`\n"
      else:
          fontes_texto += "- Nenhuma fonte recuperada.\n"

      # combina a resposta e as fontes em um √∫nico Markdown
      output_text = resposta_formatada + fontes_texto
      return output_text

  except Exception as e:
      erro_msg = f"Erro durante a invoca√ß√£o da cadeia QA: {e}"
      print(erro_msg)
      return erro_msg

# configura√ß√µes do gradio
with gr.Blocks(title="T√∫lioAI") as demo:
    gr.Markdown("# T√∫lioAI")
    gr.Markdown("Fa√ßa uma pergunta baseada nos seus arquivos de documenta√ß√£o.")

    chatbot = gr.Chatbot(
        label="Hist√≥rico da Conversa",
        type="messages", # Especifica o novo formato
        avatar_images=("https://cdn-icons-png.flaticon.com/512/4712/4712035.png", # do usu√°rio
                      "https://cdn-icons-png.flaticon.com/512/1049/1049904.png"), # do bot
        height="500px", # define uma altura fixa para o chatbot
        scale=1 # permite que o chatbot ocupe o espa√ßo dispon√≠vel
    )

    with gr.Row():
        user_input = gr.Textbox(
            label="Sua Pergunta",
            placeholder="Digite sua pergunta aqui...",
            container=False,
            scale=10
        )
        submit_btn = gr.Button("Enviar", scale=1)

    def add_message_only_user(chat_history, message):
        """
        Adiciona apenas a mensagem do usu√°rio ao hist√≥rico do chatbot.
        Retorna o hist√≥rico atualizado e limpa o campo de input.
        """
        print(f"[DEBUG - add] Recebeu chat_history com {len(chat_history) if chat_history else 0} mensagens.")
        updated_history = chat_history.copy() if chat_history else []
        if not message.strip(): # verifica se a mensagem est√° vazia ou s√≥ com espa√ßos
            print("[DEBUG - add] Mensagem vazia, retornando sem altera√ß√µes.")
            return updated_history, "" # retorna o hist√≥rico inalterado e limpa o input

        # adiciona a mensagem do usu√°rio (formatada como conte√∫do Markdown)
        updated_history.append({"role": "user", "content": f"**Voc√™**: {message}"})
        print(f"[DEBUG - add] Atualizou history para {len(updated_history)} mensagens. √öltimas 2: {updated_history[-2:] if len(updated_history) >= 2 else updated_history}")
        # retorna o hist√≥rico atualizado e uma string vazia para limpar o input
        return updated_history, ""

    def bot(chat_history):
        """
        Processa a √∫ltima pergunta do usu√°rio no hist√≥rico do chatbot e adiciona a resposta real.
        Retorna o hist√≥rico atualizado.
        """
        print(f"[DEBUG - bot] Recebeu chat_history com {len(chat_history) if chat_history else 0} mensagens. √öltimas 2: {chat_history[-2:] if chat_history and len(chat_history) >= 2 else chat_history}")

        updated_history = chat_history.copy() if chat_history else []

        # verifica se o hist√≥rico tem pelo menos uma mensagem do usu√°rio
        if len(updated_history) < 1:
            print(f"[DEBUG - bot] Hist√≥rico muito curto ({len(updated_history)} mensagens). Retornando como est√°.")
            return updated_history

        # a √∫ltima mensagem deve ser a do usu√°rio
        last_user_message_dict = updated_history[-1]

        # verifica se o papel da √∫ltima mensagem √© "user"
        if last_user_message_dict.get("role") != "user":
            print(f"[DEBUG - bot] √öltima mensagem n√£o √© do usu√°rio. Valor: {last_user_message_dict}")
            return updated_history

        # extrai a pergunta original do usu√°rio da mensagem formatada
        full_user_content = last_user_message_dict.get("content", "")
        original_user_message = full_user_content.replace("**Voc√™**: ", "", 1).strip()

        # gera a resposta real usando a fun√ß√£o definida
        bot_response = perguntar_interface(original_user_message)

        # formata a resposta final como conte√∫do Markdown
        formatted_bot_response = f"**T√∫lioAI**: {bot_response}"

        # adiciona a mensagem do assistente com a resposta real ao hist√≥rico
        updated_history.append({"role": "assistant", "content": formatted_bot_response})
        print(f"[DEBUG - bot] Adicionou resposta. Hist√≥rico final tem {len(updated_history)} mensagens.")
        return updated_history


    # liga os eventos
    submit_event = submit_btn.click(
        fn=add_message_only_user,         # Fun√ß√£o para adicionar apenas a mensagem do usu√°rio
        inputs=[chatbot, user_input],     # Inputs: hist√≥rico atual do chatbot e mensagem do usu√°rio
        outputs=[chatbot, user_input],    # Outputs: atualiza o chatbot e limpa o input
        queue=False,                      # Executa imediatamente para mostrar a pergunta
    ).then(
        fn=bot,                           # Fun√ß√£o para gerar resposta e adicionar ao hist√≥rico
        inputs=[chatbot],                 # Input: hist√≥rico atual do chatbot (com a pergunta)
        outputs=[chatbot],                # Output: atualiza o chatbot com a resposta final
        queue=True,                       # Executa em segundo plano ap√≥s a pergunta ser exibida
    )

    user_input.submit(
        fn=add_message_only_user,
        inputs=[chatbot, user_input],
        outputs=[chatbot, user_input],
        queue=False,
    ).then(
        fn=bot,
        inputs=[chatbot],
        outputs=[chatbot],
        queue=True,
    )

# share=True cria um link p√∫blico tempor√°rio
demo.launch(share=True, debug=True)